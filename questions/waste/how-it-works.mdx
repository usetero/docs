---
title: "How Tero evaluates waste"
sidebarTitle: "How it works"
description: "The method behind waste analysis"
icon: "microscope"
iconType: "duotone"
---

You've probably done manual waste analysis. Before a renewal, or when your boss wanted a service's log costs down. You got picked because you understand the service: what it does, where it sits in your architecture, which services depend on it, how it breaks. When you look at a log, you can assess whether it's waste. Not binary. A gradient. Obvious garbage is easy. Nuanced calls require understanding the service, the event, the situation where someone might actually need it.

But it's slow, manual, and has opportunity cost. You do it quarterly at best, limited to the obvious stuff. The rest keeps costing you.

## How Tero does it

You can't evaluate billions of raw logs. Neither can we. So we start by compressing them.

Every raw log maps to a [semantic log event](/context/log-events). Billions of lines become thousands of events with meaning attached. Each event inherits context from your [service catalog](/context/services): what the service does, where it sits in your architecture, which services depend on it, known failure modes. The same context you carry in your head, but explicit, complete, and always current.

Then we pressure test each event through the Tero Gauntlet.

### The Tero Gauntlet

A series of increasingly nuanced evaluations. Each stage isn't a simple check, it's reasoned analysis against everything we know about this event, this service, and your system. Fail early and you're out. Survive and we keep testing.

<Steps>
  <Step title="Drop: Garbage">
    Is this obviously worthless? Malformed data, binary blobs, debug markers left in production. If it conveys nothing, drop it.
  </Step>
  <Step title="Drop: Intrinsic value">
    Does this log carry meaningful information on its own? A payment failure tells you something. A cache hit might not. No intrinsic value? Drop it.
  </Step>
  <Step title="Drop: Extrinsic value">
    Does it contribute to the story of the transaction it's part of? A log in isolation might seem low-value, but in context of the trace, it might be the missing piece. We evaluate logs within their correlated set, not in isolation.
  </Step>
  <Step title="Drop: Failure scenarios">
    For each known failure mode of this service, would this log help investigate it? We test against a [catalog of failure scenarios](/context/services) per service. If it wouldn't help any investigation, drop it.
  </Step>
  <Step title="Convert: Metric-shaped logs">
    Is this log actually a metric in disguise? Counters, gauges, aggregations created as log lines. You're paying log prices for metric data. Convert to actual metrics and we're done.
  </Step>
  <Step title="Sample: Signal or noise">
    If this fires 1000 times, is that 1000 distinct situations or 1000 repetitions of one problem? Repetition gets sampled, but correlated sampling, by trace ID or request ID. All logs from the same request stay together or drop together.
  </Step>
  <Step title="Trim: Attribute analysis">
    For logs we keep, we analyze attributes. Duplicate values, static metadata, low-value fields. Trim the noise, keep the event.
  </Step>
</Steps>

This is deliberate. Your manual analysis can't do this at scale. Ours runs continuously, across every log event.

<Note>
#### What about security and compliance?

The same gauntlet, different lens. A log that's waste for observability might be critical for security investigations or audit requirements. Tero can evaluate purposes separately (your security team analyzing logs independently from your platform team) or combined, applying different policies at different points in your data path. Filter what goes to your SIEM differently from what goes to Datadog.
</Note>

## Categories

After analyzing billions of logs, patterns emerge. Certain types of waste keep showing up. We call these categories, but they're not limits. We don't start with categories. We observe them after analysis completes. Categories are how we organize the output so you can understand it, control it, and build trust incrementally.

<AccordionGroup>
  <Accordion title="Drop: Malformed data" icon="file-circle-xmark" iconType="duotone">
    Binary blobs, corrupted output, unparseable strings. `fdsfdsfdseef`. Not valid log data. Safe to drop entirely.
  </Accordion>
  <Accordion title="Drop: Stray debugging" icon="bug" iconType="duotone">
    Developer markers that shouldn't have shipped. `GOT HERE!`, `TODO: remove this`, print statements left behind. Safe to drop entirely.
  </Accordion>
  <Accordion title="Drop: Health checks" icon="heart-pulse" iconType="duotone">
    Readiness probes, liveness probes, synthetic monitoring. Infrastructure verifying your service is up. Safe to drop. (Failures are different — those have investigative value.)
  </Accordion>
  <Accordion title="Drop: Bot traffic" icon="robot" iconType="duotone">
    Crawlers, scrapers, automated scanners. External systems hitting your service, not real users. Safe to drop.
  </Accordion>
  <Accordion title="Convert: Logs as metrics" icon="chart-line" iconType="duotone">
    Logs created for graphing or counting. Counters, gauges, aggregations. You're paying log prices for metric data. Convert to actual metrics.
  </Accordion>
  <Accordion title="Sample: Repetitive events" icon="volume-high" iconType="duotone">
    High volume, low variance. Cache hits, heartbeats, routine operations. Also error floods during outages — a thousand timeout errors are symptoms of one problem, not a thousand investigations. Sample to reduce volume without losing visibility.
  </Accordion>
  <Accordion title="Trim: Redundant fields" icon="copy" iconType="duotone">
    The log matters, some fields don't. Stack traces when the error code tells the story. Full request bodies when the endpoint is enough. Trim the field, keep the event.
  </Accordion>
  <Accordion title="Trim: Static fields" icon="tag" iconType="duotone">
    SDK versions, infrastructure plumbing, values that never change and never help debugging. Trim the field, keep the event.
  </Accordion>
  <Accordion title="Unclassified" icon="question" iconType="duotone">
    Novel patterns that don't fit existing categories. Reviewed by our team, visible to you, a bit more experimental. As patterns become common, they graduate to named categories.
  </Accordion>
</AccordionGroup>

Not everything is waste. A payment failure with a different user each time? 1000 customers with 1000 problems. Keep all of them. Business events, customer-specific errors, state changes. Each instance is distinct. Tero identifies these and leaves them alone.

### Building trust

You don't have to engage with all of this complexity. Categories map to confidence levels. Junk and debug output are high confidence, safe starting points. Sampling and trimming require more judgment.

Start with the obvious. As you build trust, automate categories you're confident in. Push this to your teams so they own their own data quality. Set quality SLOs now that you can actually quantify them. Watch your data quality improve over time.

## Continuous

This runs all the time. Not when someone remembers. Not before a renewal panic. Every log, evaluated against the full context of your system, continuously.
