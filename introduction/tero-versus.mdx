---
title: "How Tero compares"
sidebarTitle: "Tero versus"
description: "Where Tero fits in your stack"
icon: "code-compare"
iconType: "duotone"
---

Tero is a control plane. It sits above your existing tools and makes them better. It doesn't replace anything.

## Observability platforms

**Datadog, Splunk, New Relic, Grafana**

These are where your data lives. They store it, query it, visualize it. Tero connects via API and builds understanding on top.

Your platform knows the format of your data. Tero knows the meaning: which logs matter during incidents, which metrics are never queried, what's waste and what's valuable. That understanding becomes policies you can enforce.

Swap platforms, add new ones, run multiple. Tero works across all of them. The policies are portable.

## Pipelines

**Cribl, Vector, Fluent Bit, OTel Collector**

Pipelines move data. They route, transform, filter. They're infrastructure.

The challenge is knowing what to write. You're guessing at what's safe to drop, maintaining regex lists that break when systems change, becoming a bottleneck for every filtering decision.

Tero generates the policies. You review and approve. They execute wherever you want: in the pipeline, at the edge, in your provider, or in your code. The understanding lives in Tero. The execution lives wherever makes sense.

If you don't have a pipeline, you might not need one. If you do, Tero makes it smarter.

## AI-powered pipelines

A newer category: pipelines with ML on top. They sit in the data path, sample your telemetry, use machine learning to decide what to drop, and store raw data in S3 for replay.

The pitch is automatic cost reduction. The problem is confidence.

When an engineer asks "where's my log?" you can't answer — it's somewhere in the black box. When something breaks, you can't audit what was dropped or why. "We can replay the data" isn't much comfort when you're in the middle of an incident. Your job is to make observability invisible, to keep engineers moving fast. Backfilling logs because ML guessed wrong is a failure mode, not a feature.

There's a deeper issue: sampling doesn't give you the full picture. You get recommendations, reduce some volume, but you still don't know where you stand. What's all your data? What percentage is waste? Which specific things should change? Sampling can't answer these.

Tero takes a census, not a sample. We don't sit in your data path. We connect read-only, analyze patterns across all your telemetry, and build a complete catalog. Every policy is visible, auditable, portable. Nothing is silently dropped by a model you can't inspect.

## Cost tools

**CloudZero, Vantage**

Cost tools show you what you're spending. Dashboards, reports, breakdowns by team or service.

Tero shows you what's wrong with your data and how to fix it. Costs drop as a consequence of cleaning up the waste. The goal isn't a better dashboard. It's better data.

## AI SREs

Copilots that help debug incidents, answer questions about your systems, and automate runbooks.

They're useful for what they do. But ask: what percentage of my data is waste? They can't tell you. That question requires comprehensive understanding of all your telemetry — what exists, what it means, whether it's used. Answering questions about your data isn't the same as understanding all of it.

Tero has the [Master Catalog](/master-catalog). Every question is answered against complete knowledge, not partial views.
