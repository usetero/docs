---
title: "Health checks"
description: "Readiness probes, liveness probes, synthetic monitoring"
icon: "heart-pulse"
iconType: "duotone"
---

Logs from health check endpoints. Kubernetes probes hitting `/health`, load balancers pinging `/ready`, synthetic monitors verifying uptime. Infrastructure checking if your service is alive—not real user traffic.

## Why it happens

Every production service needs health checks. Kubernetes needs to know if your pod is alive. Load balancers need to know if your instance can take traffic. Monitoring systems need to verify uptime.

Each check generates a log. Multiply by pods, by check frequency, by services. A small cluster can generate millions of health check logs per day. They tell you nothing you don't already know—if the service were unhealthy, you'd find out from the probe failures, not from reading successful probe logs.

## Example

<Tabs>
  <Tab title="Before">
    ```json
    {
      "@timestamp": "2024-01-15T10:30:00Z",
      "service.name": "checkout-api",
      "http.method": "GET",
      "http.target": "/health",
      "http.status_code": 200,
      "http.user_agent": "kube-probe/1.28"
    }
    ```
  </Tab>
  <Tab title="After">
    Dropped entirely.
  </Tab>
</Tabs>

Tero generates a scoped policy for each service where this pattern exists:

```yaml
id: drop-health-checks-checkout-api
name: Drop health check logs from checkout-api
description: Drop Kubernetes probe requests to health endpoints.
log:
  match:
    - resource_attribute: service.name
      exact: checkout-api
    - log_attribute: http.target
      regex: "^/(health|ready|live|ping)"
    - log_attribute: http.status_code
      exact: "200"
  keep: none
```

<Tip>
  Health check patterns are consistent across services. You can expand scope to apply org-wide.
</Tip>

## What about failed health checks?

Failed health checks are not flagged. A `503` response to `/health` has debugging value—it tells you when and why your service was unhealthy. Only successful probes are dropped.

## Recommended enforcement

<Card title="Enforce at edge" icon="server" href="/policies/enforcement/edge" horizontal>
  Drop health check logs before they reach your provider. Immediate volume reduction.
</Card>

Health checks are infrastructure noise. They're generated by Kubernetes and load balancers, not your application logic. Dropping them at the edge is the simplest fix.

## How it works

Tero identifies health check logs by looking at request patterns: common health check paths (`/health`, `/ready`, `/live`, `/ping`, `/healthz`), known probe user agents (`kube-probe`, `ELB-HealthChecker`), and successful status codes.

The combination of these signals makes detection reliable. A request to `/health` from `kube-probe` returning `200` is unambiguously a health check.
