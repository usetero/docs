---
title: "Privacy"
description: "What we collect, what we don't, and how you stay in control."
icon: "lock"
iconType: "duotone"
---

import { securityEmail } from '/snippets/variables.mdx';
import PrivacyChecklist from '/snippets/privacy-checklist.mdx';
import PrivacyDocuments from '/snippets/privacy-documents.mdx';
import ProgressiveTrustSteps from '/snippets/progressive-trust-steps.mdx';

<Frame>
  <img src="/images/trust-center-privacy-hero.svg" alt="Privacy" />
</Frame>

Privacy isn't a feature we added. It's built into how Tero works.

<Info>
**Looking for privacy details?** Jump to the [Reference section](#reference) at the bottom for our checklist and available documents.
</Info>

## Metadata, Not Content

The fundamental privacy decision we made was architectural: build a semantic catalog of telemetry metadata, not a warehouse of log content.

When you connect Tero to your observability platform, we analyze log samples to understand structure and patterns. We extract the schema: what fields exist, what types they are, what semantic meaning they have. This becomes the catalog entry for that log event type.

The content itself—the actual error messages, the user IDs in your logs, the request parameters—we don't store it. We process samples in memory during classification, extract the metadata, and discard the content. This isn't a limitation. It's how the product works.

This means we can tell you "this log event appears 2 million times per hour and costs $8,000 per month" without ever seeing what those 2 million logs say. The metadata is enough to deliver value.

For privacy, this changes everything. We're not asking you to trust us with your customer data or business secrets. We're not storing PII from your logs. We're building a catalog of patterns, not a database of content.

## Your Control

You control what data you share with Tero at every step. Each step is optional.

<ProgressiveTrustSteps />

## How AI Processing Works

We use AI to classify telemetry and understand quality. This requires sending log samples to an AI provider.

By default, we use Anthropic Claude. Sample logs are sent to Anthropic, processed in memory for classification, and the results are returned. The samples themselves are not persisted in Anthropic's systems or ours. This is enforced through API contracts.

<Tip>
**Need to use your approved AI provider?** Configure Tero to use AWS Bedrock, Azure OpenAI, or other providers with your API keys and compliance agreements. This works with both Tero-hosted and self-hosted deployments.
</Tip>

If you have approved AI providers, configure Tero to use them. AWS Bedrock, Azure OpenAI, or others. Use your API keys, your compliance agreements, your data residency requirements.

If you self-host the control plane, you control the entire AI pipeline. Use your models, your infrastructure, your security boundary.

The key point: AI classification happens on samples, not your full dataset. And samples are processed transiently, not stored.

## What Actually Gets Stored

To be specific about what we store:

**Account data** includes your name, email, company name. This is what you provide when creating an account. We also store authentication tokens from your SSO provider (WorkOS) for single sign-on. For self-service customers, billing information is processed through Stripe (we don't store credit card numbers, Stripe does). Enterprise customers typically pay via invoice.

**Telemetry metadata** includes schemas (field names and types), volume metrics (events per hour), quality classifications (valuable, waste, mixed), and the quality rules you define. This is what enables the semantic catalog and quality management.

**Usage data** includes which features you use, actions you take in the product, and where errors occur. Standard product analytics to improve the experience.

<Tip>
**Data residency requirements?** Self-host the control plane to keep all data in your chosen region and infrastructure. Your data never leaves your network. See [Compliance](/trust/compliance) for details.
</Tip>

We keep data while it's useful. When you delete your account or workspace, we delete the data within 30 days. Backups are retained for 30 days, then permanently deleted.

## Privacy by Design

Privacy wasn't bolted on after building the product. The semantic catalog architecture means we don't need log content to deliver value. The progressive trust model means you control access at every step. The self-hosted option means you can run everything in your network if requirements demand it.

This is privacy by design. The product works this way because we built it to respect data boundaries from the start.

## Reference

### Checklist

<PrivacyChecklist />

See the complete [Checklist](/trust/checklist) across all trust areas.

### Documents

<PrivacyDocuments />

See all available [Documents](/trust/documents) across all trust areas.

## Questions?

<Card 
  title="Contact Privacy Team" 
  icon="envelope" 
  href={`mailto:${securityEmail}`}
  horizontal
>
  Privacy questions? Data processing agreement? GDPR documentation? Email **{securityEmail}**.
</Card>
